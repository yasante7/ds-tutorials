{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0cdb342",
   "metadata": {},
   "source": [
    "# Session 2: Data Manipulation with Pandas & NumPy\n",
    "## Week 1 - Data Science & Machine Learning Training Programme\n",
    "\n",
    "**Duration:** 3 hours  \n",
    "**Learning Objectives:**\n",
    "- Master Pandas DataFrames for data manipulation\n",
    "- Understand NumPy arrays and mathematical operations\n",
    "- Learn data loading from various sources\n",
    "- Apply basic data cleaning techniques\n",
    "- Introduction to the Iris dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60239868",
   "metadata": {},
   "source": [
    "## Part 1: Advanced Pandas Operations (75 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69d2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0eeda1",
   "metadata": {},
   "source": [
    "### 1.1 Creating DataFrames from Different Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f3f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: From dictionary\n",
    "employee_data = {\n",
    "    'employee_id': [1001, 1002, 1003, 1004, 1005],\n",
    "    'name': ['Alice Johnson', 'Bob Smith', 'Charlie Brown', 'Diana Prince', 'Eve Wilson'],\n",
    "    'department': ['Engineering', 'Marketing', 'Engineering', 'Sales', 'HR'],\n",
    "    'salary': [75000, 65000, 80000, 60000, 55000],\n",
    "    'hire_date': ['2020-01-15', '2019-03-22', '2021-07-10', '2020-11-03', '2018-05-18'],\n",
    "    'performance_score': [4.2, 3.8, 4.5, 3.9, 4.1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(employee_data)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: From lists of lists\n",
    "data_lists = [\n",
    "    [1006, 'Frank Miller', 'Finance', 70000, '2021-02-28', 4.0],\n",
    "    [1007, 'Grace Lee', 'Engineering', 78000, '2020-09-15', 4.3],\n",
    "    [1008, 'Henry Davis', 'Marketing', 62000, '2021-01-20', 3.7]\n",
    "]\n",
    "\n",
    "columns = ['employee_id', 'name', 'department', 'salary', 'hire_date', 'performance_score']\n",
    "df_additional = pd.DataFrame(data_lists, columns=columns)\n",
    "\n",
    "print(\"DataFrame from lists:\")\n",
    "print(df_additional)\n",
    "\n",
    "# Combine dataframes\n",
    "df_combined = pd.concat([df, df_additional], ignore_index=True)\n",
    "print(f\"\\nCombined DataFrame shape: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d02451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Generate synthetic data (useful for practice)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 100\n",
    "synthetic_data = pd.DataFrame({\n",
    "    'customer_id': range(1001, 1001 + n_samples),\n",
    "    'age': np.random.randint(18, 65, n_samples),\n",
    "    'income': np.random.normal(50000, 15000, n_samples),\n",
    "    'purchase_amount': np.random.exponential(100, n_samples),\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], n_samples),\n",
    "    'satisfaction': np.random.uniform(1, 5, n_samples)\n",
    "})\n",
    "\n",
    "# Clean the data\n",
    "synthetic_data['income'] = synthetic_data['income'].round(2)\n",
    "synthetic_data['purchase_amount'] = synthetic_data['purchase_amount'].round(2)\n",
    "synthetic_data['satisfaction'] = synthetic_data['satisfaction'].round(1)\n",
    "\n",
    "print(\"Synthetic DataFrame:\")\n",
    "print(synthetic_data.head())\n",
    "print(f\"\\nDataFrame info:\")\n",
    "print(synthetic_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2185fa3",
   "metadata": {},
   "source": [
    "### 1.2 DataFrame Indexing and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647806af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with the employee data\n",
    "df = df_combined.copy()\n",
    "\n",
    "# Convert hire_date to datetime\n",
    "df['hire_date'] = pd.to_datetime(df['hire_date'])\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e854fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column selection\n",
    "print(\"=== Column Selection ===\")\n",
    "\n",
    "# Single column (returns Series)\n",
    "names = df['name']\n",
    "print(f\"Names (Series): {type(names)}\")\n",
    "print(names.head())\n",
    "\n",
    "# Single column (returns DataFrame)\n",
    "names_df = df[['name']]\n",
    "print(f\"\\nNames (DataFrame): {type(names_df)}\")\n",
    "print(names_df.head())\n",
    "\n",
    "# Multiple columns\n",
    "subset = df[['name', 'department', 'salary']]\n",
    "print(\"\\nMultiple columns:\")\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46863d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row selection\n",
    "print(\"=== Row Selection ===\")\n",
    "\n",
    "# By index position (.iloc)\n",
    "print(\"First 3 rows (.iloc):\")\n",
    "print(df.iloc[:3])\n",
    "\n",
    "# By label (.loc) - first set index\n",
    "df_indexed = df.set_index('employee_id')\n",
    "print(\"\\nRow with employee_id 1003 (.loc):\")\n",
    "print(df_indexed.loc[1003])\n",
    "\n",
    "# Boolean indexing (very important!)\n",
    "high_performers = df[df['performance_score'] >= 4.0]\n",
    "print(f\"\\nHigh performers (score >= 4.0): {len(high_performers)} employees\")\n",
    "print(high_performers[['name', 'performance_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced filtering\n",
    "print(\"=== Advanced Filtering ===\")\n",
    "\n",
    "# Multiple conditions with &, |, ~\n",
    "high_paid_engineers = df[(df['department'] == 'Engineering') & (df['salary'] > 70000)]\n",
    "print(\"High-paid Engineers:\")\n",
    "print(high_paid_engineers[['name', 'salary']])\n",
    "\n",
    "# Using .isin() for multiple values\n",
    "tech_sales = df[df['department'].isin(['Engineering', 'Sales'])]\n",
    "print(f\"\\nEngineering or Sales employees: {len(tech_sales)}\")\n",
    "\n",
    "# String operations\n",
    "j_names = df[df['name'].str.startswith('J')]\n",
    "print(\"\\nEmployees whose names start with 'J':\")\n",
    "print(j_names['name'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d1606",
   "metadata": {},
   "source": [
    "### 1.3 Data Aggregation and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f334740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic aggregations\n",
    "print(\"=== Basic Aggregations ===\")\n",
    "print(f\"Average salary: ${df['salary'].mean():,.2f}\")\n",
    "print(f\"Median salary: ${df['salary'].median():,.2f}\")\n",
    "print(f\"Salary standard deviation: ${df['salary'].std():,.2f}\")\n",
    "print(f\"Total employees: {len(df)}\")\n",
    "\n",
    "# Descriptive statistics\n",
    "print(\"\\nDescriptive statistics for numerical columns:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb02fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy operations (very important for data analysis)\n",
    "print(\"=== GroupBy Operations ===\")\n",
    "\n",
    "# Group by department\n",
    "dept_stats = df.groupby('department').agg({\n",
    "    'salary': ['mean', 'median', 'count'],\n",
    "    'performance_score': 'mean'\n",
    "})\n",
    "\n",
    "print(\"Statistics by department:\")\n",
    "print(dept_stats)\n",
    "\n",
    "# Flatten column names for easier access\n",
    "dept_stats.columns = ['_'.join(col).strip() for col in dept_stats.columns]\n",
    "print(\"\\nWith flattened column names:\")\n",
    "print(dept_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More groupby examples\n",
    "print(\"=== Advanced GroupBy ===\")\n",
    "\n",
    "# Create salary bands\n",
    "df['salary_band'] = pd.cut(df['salary'], \n",
    "                          bins=[0, 60000, 70000, 80000, float('inf')],\n",
    "                          labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"Salary bands:\")\n",
    "print(df[['name', 'salary', 'salary_band']])\n",
    "\n",
    "# Cross-tabulation\n",
    "cross_tab = pd.crosstab(df['department'], df['salary_band'])\n",
    "print(\"\\nCross-tabulation of department vs salary band:\")\n",
    "print(cross_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bbee2",
   "metadata": {},
   "source": [
    "## Part 2: NumPy Arrays and Mathematical Operations (45 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c160d",
   "metadata": {},
   "source": [
    "### 2.1 NumPy Array Creation and Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f8fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways to create arrays\n",
    "print(\"=== Array Creation ===\")\n",
    "\n",
    "# From lists\n",
    "arr1 = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"From list: {arr1}\")\n",
    "print(f\"Type: {type(arr1)}, Shape: {arr1.shape}, Dtype: {arr1.dtype}\")\n",
    "\n",
    "# 2D array\n",
    "arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(f\"\\n2D array:\\n{arr2d}\")\n",
    "print(f\"Shape: {arr2d.shape}, Dimensions: {arr2d.ndim}\")\n",
    "\n",
    "# Using built-in functions\n",
    "zeros = np.zeros((3, 4))\n",
    "ones = np.ones((2, 3))\n",
    "identity = np.eye(3)\n",
    "print(f\"\\nZeros array:\\n{zeros}\")\n",
    "print(f\"\\nIdentity matrix:\\n{identity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb1e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range-based arrays\n",
    "print(\"=== Range-based Arrays ===\")\n",
    "\n",
    "# arange: similar to range() but returns array\n",
    "range_arr = np.arange(0, 10, 2)\n",
    "print(f\"arange(0, 10, 2): {range_arr}\")\n",
    "\n",
    "# linspace: evenly spaced numbers\n",
    "linear = np.linspace(0, 1, 5)\n",
    "print(f\"linspace(0, 1, 5): {linear}\")\n",
    "\n",
    "# Random arrays\n",
    "np.random.seed(42)\n",
    "random_uniform = np.random.uniform(0, 1, 5)\n",
    "random_normal = np.random.normal(0, 1, 5)\n",
    "print(f\"Random uniform: {random_uniform}\")\n",
    "print(f\"Random normal: {random_normal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e95f84",
   "metadata": {},
   "source": [
    "### 2.2 Array Operations and Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise operations\n",
    "print(\"=== Element-wise Operations ===\")\n",
    "\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.array([2, 3, 4, 5, 6])\n",
    "\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"a + b: {a + b}\")\n",
    "print(f\"a * b: {a * b}\")\n",
    "print(f\"a ** 2: {a ** 2}\")\n",
    "print(f\"np.sqrt(a): {np.sqrt(a)}\")\n",
    "\n",
    "# Comparison operations\n",
    "print(f\"\\na > 3: {a > 3}\")\n",
    "print(f\"a[a > 3]: {a[a > 3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8274c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting - NumPy's powerful feature\n",
    "print(\"=== Broadcasting ===\")\n",
    "\n",
    "# Scalar with array\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "result1 = arr + 10\n",
    "print(f\"Array + scalar: {arr} + 10 = {result1}\")\n",
    "\n",
    "# Arrays with different shapes\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "vector = np.array([10, 20, 30])\n",
    "\n",
    "print(f\"\\nMatrix:\\n{matrix}\")\n",
    "print(f\"Vector: {vector}\")\n",
    "print(f\"Matrix + Vector:\\n{matrix + vector}\")\n",
    "\n",
    "# Column vector broadcasting\n",
    "col_vector = np.array([[100], [200]])\n",
    "print(f\"\\nColumn vector:\\n{col_vector}\")\n",
    "print(f\"Matrix + Column vector:\\n{matrix + col_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical operations\n",
    "print(\"=== Statistical Operations ===\")\n",
    "\n",
    "data = np.random.normal(100, 15, (4, 5))  # 4x5 matrix\n",
    "print(f\"Data matrix (4x5):\\n{data.round(2)}\")\n",
    "\n",
    "print(f\"\\nOverall statistics:\")\n",
    "print(f\"Mean: {data.mean():.2f}\")\n",
    "print(f\"Std: {data.std():.2f}\")\n",
    "print(f\"Min: {data.min():.2f}\")\n",
    "print(f\"Max: {data.max():.2f}\")\n",
    "\n",
    "print(f\"\\nAxis-wise operations:\")\n",
    "print(f\"Mean along axis 0 (columns): {data.mean(axis=0).round(2)}\")\n",
    "print(f\"Mean along axis 1 (rows): {data.mean(axis=1).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0ade1",
   "metadata": {},
   "source": [
    "### 2.3 Array Reshaping and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping arrays\n",
    "print(\"=== Array Reshaping ===\")\n",
    "\n",
    "original = np.arange(12)\n",
    "print(f\"Original array: {original}\")\n",
    "print(f\"Shape: {original.shape}\")\n",
    "\n",
    "# Reshape to 2D\n",
    "reshaped = original.reshape(3, 4)\n",
    "print(f\"\\nReshaped (3x4):\\n{reshaped}\")\n",
    "\n",
    "# Reshape to 3D\n",
    "reshaped_3d = original.reshape(2, 2, 3)\n",
    "print(f\"\\nReshaped (2x2x3):\\n{reshaped_3d}\")\n",
    "\n",
    "# Flatten back to 1D\n",
    "flattened = reshaped.flatten()\n",
    "print(f\"\\nFlattened: {flattened}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f6e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced indexing\n",
    "print(\"=== Advanced Indexing ===\")\n",
    "\n",
    "matrix = np.arange(20).reshape(4, 5)\n",
    "print(f\"Matrix (4x5):\\n{matrix}\")\n",
    "\n",
    "# Slicing\n",
    "print(f\"\\nFirst two rows, first three columns:\\n{matrix[:2, :3]}\")\n",
    "print(f\"\\nLast row: {matrix[-1, :]}\")\n",
    "print(f\"\\nLast column: {matrix[:, -1]}\")\n",
    "\n",
    "# Boolean indexing\n",
    "mask = matrix > 10\n",
    "print(f\"\\nElements > 10: {matrix[mask]}\")\n",
    "\n",
    "# Fancy indexing\n",
    "rows = [0, 2]\n",
    "cols = [1, 3, 4]\n",
    "print(f\"\\nSelected elements (rows {rows}, cols {cols}):\\n{matrix[np.ix_(rows, cols)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928e2ac",
   "metadata": {},
   "source": [
    "## Part 3: Data Loading from Various Sources (30 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1fc7f4",
   "metadata": {},
   "source": [
    "### 3.1 Loading Data from Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample CSV data first\n",
    "sample_data = {\n",
    "    'product_id': range(1, 101),\n",
    "    'product_name': [f'Product_{i}' for i in range(1, 101)],\n",
    "    'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 100),\n",
    "    'price': np.random.uniform(10, 500, 100).round(2),\n",
    "    'stock_quantity': np.random.randint(0, 100, 100),\n",
    "    'rating': np.random.uniform(1, 5, 100).round(1)\n",
    "}\n",
    "\n",
    "df_products = pd.DataFrame(sample_data)\n",
    "\n",
    "# Save to CSV (we'll load it back)\n",
    "df_products.to_csv('sample_products.csv', index=False)\n",
    "print(\"Sample CSV file created: sample_products.csv\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nSample data:\")\n",
    "print(df_products.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV files\n",
    "print(\"=== Loading CSV Files ===\")\n",
    "\n",
    "# Basic loading\n",
    "df_loaded = pd.read_csv('sample_products.csv')\n",
    "print(f\"Loaded DataFrame shape: {df_loaded.shape}\")\n",
    "print(f\"Columns: {list(df_loaded.columns)}\")\n",
    "\n",
    "# Loading with specific parameters\n",
    "df_custom = pd.read_csv('sample_products.csv', \n",
    "                       usecols=['product_name', 'category', 'price'],\n",
    "                       nrows=10)\n",
    "print(\"\\nLoaded with custom parameters (first 10 rows, selected columns):\")\n",
    "print(df_custom)\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df_loaded.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0db65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load JSON data\n",
    "import json\n",
    "\n",
    "# Create sample JSON\n",
    "json_data = {\n",
    "    'customers': [\n",
    "        {'id': 1, 'name': 'John Doe', 'email': 'john@email.com', 'orders': [101, 102]},\n",
    "        {'id': 2, 'name': 'Jane Smith', 'email': 'jane@email.com', 'orders': [103]},\n",
    "        {'id': 3, 'name': 'Bob Johnson', 'email': 'bob@email.com', 'orders': [104, 105, 106]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('sample_customers.json', 'w') as f:\n",
    "    json.dump(json_data, f)\n",
    "\n",
    "# Load JSON data\n",
    "df_json = pd.read_json('sample_customers.json')\n",
    "print(\"JSON data loaded:\")\n",
    "print(df_json)\n",
    "\n",
    "# Normalize nested JSON\n",
    "from pandas import json_normalize\n",
    "df_normalized = json_normalize(json_data['customers'])\n",
    "print(\"\\nNormalized JSON data:\")\n",
    "print(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafc225",
   "metadata": {},
   "source": [
    "### 3.2 Loading from URLs and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from URL (if internet connection available)\n",
    "try:\n",
    "    # Example: Loading Iris dataset from UCI repository\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    \n",
    "    # Column names for Iris dataset\n",
    "    iris_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "    \n",
    "    iris_df = pd.read_csv(url, names=iris_columns)\n",
    "    print(\"Iris dataset loaded from URL:\")\n",
    "    print(iris_df.head())\n",
    "    print(f\"Shape: {iris_df.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load from URL: {e}\")\n",
    "    print(\"This might be due to internet connectivity. We'll create a local Iris dataset instead.\")\n",
    "    \n",
    "    # Create local Iris-like dataset\n",
    "    from sklearn.datasets import load_iris\n",
    "    iris_sklearn = load_iris()\n",
    "    \n",
    "    iris_df = pd.DataFrame(iris_sklearn.data, columns=iris_sklearn.feature_names)\n",
    "    iris_df['species'] = iris_sklearn.target_names[iris_sklearn.target]\n",
    "    print(\"\\nLocal Iris dataset created:\")\n",
    "    print(iris_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4880e",
   "metadata": {},
   "source": [
    "## Part 4: Introduction to the Iris Dataset (30 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6152f8e",
   "metadata": {},
   "source": [
    "### 4.1 Understanding the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the complete Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load data\n",
    "iris_data = load_iris()\n",
    "iris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "iris_df['species'] = iris_data.target_names[iris_data.target]\n",
    "iris_df['target'] = iris_data.target\n",
    "\n",
    "print(\"=== Iris Dataset Overview ===\")\n",
    "print(f\"Dataset shape: {iris_df.shape}\")\n",
    "print(f\"Features: {iris_data.feature_names}\")\n",
    "print(f\"Classes: {iris_data.target_names}\")\n",
    "print(f\"\\nDataset description:\")\n",
    "print(iris_data.DESCR[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d45e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "print(\"=== Dataset Exploration ===\")\n",
    "print(\"First 5 rows:\")\n",
    "print(iris_df.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(iris_df.tail())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(iris_df.info())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(iris_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167dde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore species distribution\n",
    "print(\"=== Species Analysis ===\")\n",
    "species_counts = iris_df['species'].value_counts()\n",
    "print(\"Species distribution:\")\n",
    "print(species_counts)\n",
    "\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print((species_counts / len(iris_df) * 100).round(2))\n",
    "\n",
    "# Statistics by species\n",
    "print(\"\\nMean values by species:\")\n",
    "species_stats = iris_df.groupby('species').mean()\n",
    "print(species_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74394685",
   "metadata": {},
   "source": [
    "### 4.2 Basic Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for the Iris dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Iris Dataset - Basic Visualizations', fontsize=16)\n",
    "\n",
    "# Histogram of sepal length\n",
    "axes[0, 0].hist(iris_df['sepal length (cm)'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Sepal Length Distribution')\n",
    "axes[0, 0].set_xlabel('Sepal Length (cm)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Box plot of petal length by species\n",
    "species_data = [iris_df[iris_df['species'] == species]['petal length (cm)'] \n",
    "                for species in iris_df['species'].unique()]\n",
    "axes[0, 1].boxplot(species_data, labels=iris_df['species'].unique())\n",
    "axes[0, 1].set_title('Petal Length by Species')\n",
    "axes[0, 1].set_ylabel('Petal Length (cm)')\n",
    "\n",
    "# Scatter plot\n",
    "for species in iris_df['species'].unique():\n",
    "    species_data = iris_df[iris_df['species'] == species]\n",
    "    axes[1, 0].scatter(species_data['sepal length (cm)'], \n",
    "                       species_data['sepal width (cm)'], \n",
    "                       label=species, alpha=0.7)\n",
    "axes[1, 0].set_title('Sepal Length vs Width')\n",
    "axes[1, 0].set_xlabel('Sepal Length (cm)')\n",
    "axes[1, 0].set_ylabel('Sepal Width (cm)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Bar chart of species counts\n",
    "species_counts.plot(kind='bar', ax=axes[1, 1], color=['coral', 'lightgreen', 'lightblue'])\n",
    "axes[1, 1].set_title('Species Count')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced visualization with Seaborn\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Pairplot to show all feature relationships\n",
    "sns.pairplot(iris_df, hue='species', markers=[\"o\", \"s\", \"D\"])\n",
    "plt.suptitle('Iris Dataset - Pairwise Feature Relationships', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "numeric_cols = iris_df.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_cols.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf18ad",
   "metadata": {},
   "source": [
    "## Part 5: Basic Data Cleaning Techniques (30 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5846c",
   "metadata": {},
   "source": [
    "### 5.1 Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with missing values for practice\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "messy_data = pd.DataFrame({\n",
    "    'name': [f'Person_{i}' for i in range(n_samples)],\n",
    "    'age': np.random.randint(18, 65, n_samples),\n",
    "    'income': np.random.normal(50000, 15000, n_samples),\n",
    "    'score': np.random.uniform(0, 100, n_samples)\n",
    "})\n",
    "\n",
    "# Introduce missing values\n",
    "missing_indices = np.random.choice(n_samples, size=10, replace=False)\n",
    "messy_data.loc[missing_indices[:5], 'age'] = np.nan\n",
    "messy_data.loc[missing_indices[5:], 'income'] = np.nan\n",
    "\n",
    "print(\"=== Dataset with Missing Values ===\")\n",
    "print(f\"Dataset shape: {messy_data.shape}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(messy_data.isnull().sum())\n",
    "\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(messy_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d96409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different strategies for handling missing data\n",
    "print(\"=== Missing Data Strategies ===\")\n",
    "\n",
    "# Strategy 1: Drop rows with any missing values\n",
    "clean_drop = messy_data.dropna()\n",
    "print(f\"After dropping rows with missing values: {clean_drop.shape}\")\n",
    "\n",
    "# Strategy 2: Fill with mean/median\n",
    "clean_fill = messy_data.copy()\n",
    "clean_fill['age'].fillna(clean_fill['age'].median(), inplace=True)\n",
    "clean_fill['income'].fillna(clean_fill['income'].mean(), inplace=True)\n",
    "\n",
    "print(f\"After filling with mean/median: {clean_fill.shape}\")\n",
    "print(f\"Missing values after filling: {clean_fill.isnull().sum().sum()}\")\n",
    "\n",
    "# Strategy 3: Forward fill (useful for time series)\n",
    "clean_ffill = messy_data.fillna(method='ffill')\n",
    "print(f\"Missing values after forward fill: {clean_ffill.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10220db2",
   "metadata": {},
   "source": [
    "### 5.2 Data Type Conversions and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a32a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with mixed data types\n",
    "mixed_data = pd.DataFrame({\n",
    "    'id': ['001', '002', '003', '004', '005'],\n",
    "    'price': ['10.50', '25.99', 'N/A', '15.75', '30.00'],\n",
    "    'quantity': ['5', '10', '3', '7', '12'],\n",
    "    'date': ['2023-01-15', '2023-02-20', '2023-03-10', '2023-04-05', '2023-05-12'],\n",
    "    'is_active': ['True', 'False', 'True', 'True', 'False']\n",
    "})\n",
    "\n",
    "print(\"=== Data Type Conversion ===\")\n",
    "print(\"Original data types:\")\n",
    "print(mixed_data.dtypes)\n",
    "print(\"\\nOriginal data:\")\n",
    "print(mixed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e873967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types\n",
    "cleaned_data = mixed_data.copy()\n",
    "\n",
    "# Convert price to numeric (handle 'N/A')\n",
    "cleaned_data['price'] = pd.to_numeric(cleaned_data['price'], errors='coerce')\n",
    "\n",
    "# Convert quantity to integer\n",
    "cleaned_data['quantity'] = cleaned_data['quantity'].astype(int)\n",
    "\n",
    "# Convert date to datetime\n",
    "cleaned_data['date'] = pd.to_datetime(cleaned_data['date'])\n",
    "\n",
    "# Convert boolean string to boolean\n",
    "cleaned_data['is_active'] = cleaned_data['is_active'].map({'True': True, 'False': False})\n",
    "\n",
    "print(\"After conversion:\")\n",
    "print(cleaned_data.dtypes)\n",
    "print(\"\\nCleaned data:\")\n",
    "print(cleaned_data)\n",
    "print(f\"\\nMissing values: {cleaned_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080ec7c",
   "metadata": {},
   "source": [
    "### 5.3 Practical Exercise: Clean the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378cec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's artificially introduce some issues to the Iris dataset for cleaning practice\n",
    "iris_messy = iris_df.copy()\n",
    "\n",
    "# Introduce some issues\n",
    "# 1. Add some missing values\n",
    "iris_messy.loc[10:12, 'sepal length (cm)'] = np.nan\n",
    "\n",
    "# 2. Add some outliers\n",
    "iris_messy.loc[5, 'petal length (cm)'] = 100  # Unrealistic value\n",
    "\n",
    "# 3. Add inconsistent species names\n",
    "iris_messy.loc[20:22, 'species'] = 'SETOSA'  # Uppercase\n",
    "iris_messy.loc[80:82, 'species'] = 'versicolor '  # Extra space\n",
    "\n",
    "print(\"=== Iris Dataset Cleaning Exercise ===\")\n",
    "print(\"Issues introduced:\")\n",
    "print(f\"Missing values: {iris_messy.isnull().sum().sum()}\")\n",
    "print(f\"Unique species (should be 3): {iris_messy['species'].nunique()}\")\n",
    "print(f\"Species values: {iris_messy['species'].unique()}\")\n",
    "print(f\"Max petal length: {iris_messy['petal length (cm)'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f168a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataset\n",
    "iris_clean = iris_messy.copy()\n",
    "\n",
    "# 1. Handle missing values - fill with median\n",
    "for column in iris_clean.select_dtypes(include=[np.number]).columns:\n",
    "    if iris_clean[column].isnull().any():\n",
    "        median_value = iris_clean[column].median()\n",
    "        iris_clean[column].fillna(median_value, inplace=True)\n",
    "        print(f\"Filled missing values in {column} with median: {median_value:.2f}\")\n",
    "\n",
    "# 2. Handle outliers - cap extreme values\n",
    "for column in ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']:\n",
    "    Q1 = iris_clean[column].quantile(0.25)\n",
    "    Q3 = iris_clean[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = iris_clean[(iris_clean[column] < lower_bound) | (iris_clean[column] > upper_bound)]\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"Found {len(outliers)} outliers in {column}\")\n",
    "        iris_clean[column] = iris_clean[column].clip(lower_bound, upper_bound)\n",
    "\n",
    "# 3. Standardize species names\n",
    "iris_clean['species'] = iris_clean['species'].str.lower().str.strip()\n",
    "\n",
    "print(f\"\\nAfter cleaning:\")\n",
    "print(f\"Missing values: {iris_clean.isnull().sum().sum()}\")\n",
    "print(f\"Unique species: {iris_clean['species'].nunique()}\")\n",
    "print(f\"Species values: {iris_clean['species'].unique()}\")\n",
    "print(f\"Max petal length: {iris_clean['petal length (cm)'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bac85d",
   "metadata": {},
   "source": [
    "## Summary and Homework\n",
    "\n",
    "### What We Covered Today:\n",
    "1. ✅ Advanced Pandas operations: DataFrame creation, indexing, grouping, aggregation\n",
    "2. ✅ NumPy arrays: creation, operations, broadcasting, statistical functions\n",
    "3. ✅ Data loading from various sources: CSV, JSON, URLs\n",
    "4. ✅ Introduction to the Iris dataset: exploration and visualization\n",
    "5. ✅ Basic data cleaning: missing values, data types, outliers\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "- **DataFrame manipulation** with advanced indexing and filtering\n",
    "- **NumPy array operations** and broadcasting\n",
    "- **Data loading** from multiple sources\n",
    "- **Exploratory Data Analysis** techniques\n",
    "- **Data cleaning** strategies\n",
    "\n",
    "### Homework Before Next Session:\n",
    "1. **Complete the data cleaning exercise** above if not finished\n",
    "2. **Explore the Iris dataset** further - try different visualizations\n",
    "3. **Practice loading data** from different file formats (create your own CSV/JSON files)\n",
    "4. **Read about data visualization** - we'll create comprehensive EDA visualizations next session\n",
    "\n",
    "### Next Session Preview:\n",
    "**Session 3: Data Visualisation & EDA + Iris Classification Project**\n",
    "- Advanced data visualization with Matplotlib and Seaborn\n",
    "- Comprehensive EDA methodology\n",
    "- Building your first machine learning model with the Iris dataset\n",
    "- GitHub project documentation\n",
    "\n",
    "### Additional Resources:\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [NumPy Documentation](https://numpy.org/doc/)\n",
    "- [Iris Dataset Information](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "- [Data Cleaning Guide](https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
